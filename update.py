import requests
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# 1. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø°ÙƒÙŠØ© (Smart Sources)
# ==========================================
# Ù‚Ù…Ù†Ø§ Ø¨Ø¥Ø¶Ø§ÙØ© Ù…ØµØ§Ø¯Ø± ØªØªØ¬Ø¯Ø¯ ÙŠÙˆÙ…ÙŠØ§Ù‹ ÙˆØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø³ÙŠØ±ÙØ±Ø§Øª Xtream Ù…Ø­ÙˆÙ„Ø©
URLS = [
    # Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø±Ø³Ù…ÙŠØ© (Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØµÙ„Ø¨Ø©)
    "https://iptv-org.github.io/iptv/countries/jo.m3u",
    "https://iptv-org.github.io/iptv/countries/eg.m3u",
    "https://iptv-org.github.io/iptv/countries/sa.m3u",
    "https://iptv-org.github.io/iptv/languages/ara.m3u",
    
    # Ù…ØµØ§Ø¯Ø± Ø¹Ø§Ù„Ù…ÙŠØ© Ø¶Ø®Ù…Ø© (Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ MBC ÙˆØ³ÙŠØ±ÙØ±Ø§Øª Ø®Ø§ØµØ©)
    "https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8",
    "https://raw.githubusercontent.com/jnk22/kodirpo/master/iptv/arab.m3u",
    
    # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (ÙˆØ«Ø§Ø¦Ù‚ÙŠØŒ Ø£Ø·ÙØ§Ù„ØŒ Ø±ÙŠØ§Ø¶Ø©)
    "https://iptv-org.github.io/iptv/categories/documentary.m3u",
    "https://iptv-org.github.io/iptv/categories/kids.m3u",
    "https://iptv-org.github.io/iptv/categories/sports.m3u"
]

# Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙŠ Ù†Ø±ÙŠØ¯ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„ÙŠÙ‡Ø§ (VIP)
TARGET_CHANNELS = [
    "mbc", "bein", "osn", "rotana", "art ", "shahid", 
    "national geo", "nat geo", "discovery", "animal planet",
    "spacetoon", "cartoon network", "cn arabia",
    "jordan", "roya", "mamlaka", "jazeera"
]

# ==========================================
# 2. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙØ­Øµ Ø§Ù„Ø°ÙƒÙŠ
# ==========================================
# Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© (ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø§Ù„Ø±Ù‚Ù… Ø²Ø§Ø¯Øª Ø§Ù„Ø³Ø±Ø¹Ø© Ù„ÙƒÙ† Ø²Ø§Ø¯ Ø§Ù„Ø¶ØºØ·)
MAX_THREADS = 20 
# Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· Ù…ÙŠØªØ§Ù‹ (Ø«ÙˆØ§Ù†ÙŠ)
TIMEOUT = 4

# Ù‡ÙŠØ¯Ø± Ù„ØªÙ…ÙˆÙŠÙ‡ Ø§Ù„Ø·Ù„Ø¨ ÙˆÙƒØ£Ù†Ù‡ Ù…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ (Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ù…Ø§ÙŠØ©)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# ==========================================
# 3. Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø°ÙƒÙŠØ©
# ==========================================

def check_link(url):
    """
    ÙˆØ¸ÙŠÙØ© ØªÙ‚ÙˆÙ… Ø¨ÙØ­Øµ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ù„ Ù‡Ùˆ Ø­ÙŠ Ø£Ù… Ù…ÙŠØªØŸ
    ØªØ±Ø¬Ø¹ True Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø´ØºØ§Ù„ØŒ Ùˆ False Ø¥Ø°Ø§ Ø®Ø±Ø¨Ø§Ù†.
    """
    try:
        # Ù†Ø±Ø³Ù„ Ø·Ù„Ø¨ HEAD (Ø®ÙÙŠÙ Ø¬Ø¯Ø§Ù‹) Ø£Ùˆ GET Ù„Ø£ÙˆÙ„ Ø¨Ø§ÙŠØªØ§Øª ÙÙ‚Ø·
        with requests.get(url, headers=HEADERS, stream=True, timeout=TIMEOUT) as response:
            if response.status_code == 200:
                return True
    except:
        return False
    return False

def clean_name(name):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³Ù… Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‚Ù†ÙˆØ§Øª"""
    name = name.lower()
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ¤Ø«Ø± ÙÙŠ Ù‡ÙˆÙŠØ© Ø§Ù„Ù‚Ù†Ø§Ø©
    replacements = ["hd", "sd", "fhd", "4k", "hevc", "ar", "arabic", "tv", "channel", "live", "stream"]
    for word in replacements:
        name = re.sub(rf'\b{word}\b', '', name)
    return re.sub(r'[^a-z0-9]', '', name)

def get_category(name):
    """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù‚Ù†Ø§Ø© Ø°ÙƒÙŠØ§Ù‹"""
    n = name.lower()
    if "sport" in n or "koora" in n or "bein" in n: return "sports"
    if "news" in n or "jazeera" in n or "arabia" in n: return "news"
    if "kid" in n or "cartoon" in n or "spacetoon" in n: return "kids"
    if "movi" in n or "cinema" in n or "film" in n or "rotana" in n or "mbc 2" in n or "mbc action" in n: return "movies"
    if "docu" in n or "geo" in n or "wild" in n or "planet" in n: return "docu"
    return "general"

def fetch_all_channels():
    """Ø¬Ù„Ø¨ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± ÙˆØªØµÙÙŠØªÙ‡Ø§ Ù…Ø¨Ø¯Ø¦ÙŠØ§Ù‹"""
    raw_channels = []
    print("ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ø¶Ø®Ù…Ø©...")
    
    for url in URLS:
        try:
            resp = requests.get(url, timeout=10)
            resp.encoding = 'utf-8'
            lines = resp.text.split('\n')
            
            current_meta = {}
            for line in lines:
                line = line.strip()
                if line.startswith("#EXTINF"):
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³Ù…
                    name_m = re.search(r'tvg-name="([^"]+)"', line) or re.search(r',(.*)', line)
                    name = name_m.group(1).strip() if name_m else "Unknown"
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù„ÙˆØ¬Ùˆ
                    logo_m = re.search(r'tvg-logo="([^"]+)"', line)
                    logo = logo_m.group(1) if logo_m else ""
                    
                    # Ù‡Ù„ Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ù† Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©ØŸ
                    is_target = any(k in name.lower() for k in TARGET_CHANNELS)
                    
                    # Ø¥Ø°Ø§ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ø±Ø¨ÙŠØ© Ø®Ø° ÙƒÙ„ Ø´ÙŠØ¡ØŒ Ø¥Ø°Ø§ Ø¹Ø§Ù„Ù…ÙŠØ© Ø®Ø° ÙÙ‚Ø· Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
                    if "ara.m3u" in url or "jo.m3u" in url or "arab" in url or is_target:
                        current_meta = {"name": name, "logo": logo}
                    else:
                        current_meta = {}
                        
                elif line.startswith("http") and current_meta:
                    if not line.endswith(".ts"): # Ù†ØªØ¬Ù†Ø¨ Ù…Ù„ÙØ§Øª ts Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆÙ†Ø±ÙƒØ² Ø¹Ù„Ù‰ m3u8
                        raw_channels.append({
                            "name": current_meta['name'],
                            "logo": current_meta['logo'],
                            "url": line
                        })
                    current_meta = {}
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø± {url}: {e}")
            
    return raw_channels

# ==========================================
# 4. Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Main Engine)
# ==========================================

def update():
    all_raw = fetch_all_channels()
    print(f"ğŸ“¦ ØªÙ… Ø¬Ù…Ø¹ {len(all_raw)} Ø±Ø§Ø¨Ø· Ù…Ø­ØªÙ…Ù„. Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ø§Ù„ÙØ­Øµ Ø§Ù„Ø°ÙƒÙŠ (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹)...")
    
    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…
    grouped_channels = {}
    
    # 1. Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù„ÙØ­Øµ (Ø§Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…ÙƒØ±Ø±)
    unique_urls_to_check = set()
    
    for ch in all_raw:
        clean_id = clean_name(ch['name'])
        
        # ØªØµØ­ÙŠØ­ Ø®Ø§Øµ Ù„Ù€ MBC
        if "mbc" in clean_id:
            if "drama" in clean_id: clean_id = "mbcdrama"
            elif "action" in clean_id: clean_id = "mbcaction"
            elif "max" in clean_id: clean_id = "mbcmax"
            elif "bollywood" in clean_id: clean_id = "mbcbollywood"
            elif "2" in clean_id: clean_id = "mbc2"
            elif "3" in clean_id: clean_id = "mbc3"
            elif "4" in clean_id: clean_id = "mbc4"
            elif "iraq" in clean_id: clean_id = "mbciraq"
            elif "masr" in clean_id: clean_id = "mbcmasr"
        
        if clean_id not in grouped_channels:
            grouped_channels[clean_id] = {
                "name": ch['name'],
                "logo": ch['logo'],
                "category": get_category(ch['name']),
                "potential_urls": set()
            }
        
        grouped_channels[clean_id]['potential_urls'].add(ch['url'])
        unique_urls_to_check.add(ch['url'])

    print(f"ğŸ” Ù„Ø¯ÙŠÙ†Ø§ {len(unique_urls_to_check)} Ø±Ø§Ø¨Ø· ÙØ±ÙŠØ¯ Ù„Ù„ÙØ­Øµ. ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙŠØ±Ø¨Ùˆ...")

    # 2. Ø§Ù„ÙØ­Øµ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ (Multi-threading Link Checker)
    valid_urls = set()
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‡Ø§Ù…
        future_to_url = {executor.submit(check_link, url): url for url in unique_urls_to_check}
        
        completed = 0
        total = len(unique_urls_to_check)
        
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            completed += 1
            if completed % 50 == 0: print(f"â³ ØªÙ… ÙØ­Øµ {completed}/{total}...")
            
            try:
                is_working = future.result()
                if is_working:
                    valid_urls.add(url)
            except:
                pass

    print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙØ­Øµ. Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´ØºØ§Ù„Ø©: {len(valid_urls)}")

    # 3. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    final_list = []
    
    for key, data in grouped_channels.items():
        # Ù†Ø£Ø®Ø° ÙÙ‚Ø· Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ Ù†Ø¬Ø­Øª ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        working_urls = [u for u in data['potential_urls'] if u in valid_urls]
        
        if working_urls:
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª: Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
            official_urls = [u for u in working_urls if "shahid" in u or "viaplay" in u]
            other_urls = [u for u in working_urls if u not in official_urls]
            sorted_urls = official_urls + other_urls
            
            final_list.append({
                "name": data['name'],
                "logo": data['logo'],
                "category": data['category'],
                "urls": sorted_urls
            })

    # ØªØ±ØªÙŠØ¨ Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø¹Ø±Ø¶ (Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© ÙˆØ§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©)
    priority = ["Jordan", "Roya", "Mamlaka", "MBC", "Jazeera", "BeIN", "Nat Geo"]
    def sort_key(item):
        n = item['name'].lower()
        for i, p in enumerate(priority):
            if p.lower() in n: return i
        return 100

    final_list.sort(key=sort_key)

    # Ø§Ù„Ø­ÙØ¸
    with open("channels.json", "w", encoding="utf-8") as f:
        json.dump(final_list, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ‰ ØªÙ… Ø­ÙØ¸ {len(final_list)} Ù‚Ù†Ø§Ø© Ù…Ø¤ÙƒØ¯Ø© Ø§Ù„Ø¹Ù…Ù„ 100%!")

if __name__ == "__main__":
    update()

import requests
import json
import re
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# 0. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© (Advanced Settings)
# ==========================================
MAX_WORKERS = 50   # Ø²Ø¯Ù†Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ù„Ø³Ø±Ø¹Ø© ØµØ§Ø±ÙˆØ®ÙŠØ©
TIMEOUT = 6        # Ù…Ù‡Ù„Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø¹Ù‚ÙˆÙ„Ø©

# Ù‚Ø§Ø¦Ù…Ø© Ù‡ÙˆÙŠØ§Øª ÙˆÙ‡Ù…ÙŠØ© (Ù„Ù„ØªØ®ÙÙŠ ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "VLC/3.0.20 LibVLC/3.0.20"
]

# ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¸ÙˆØ±Ø© (Ù„Ø¶Ù…Ø§Ù† Ù…Ø­ØªÙˆÙ‰ Ù†Ø¸ÙŠÙ)
BLACKLIST = ["adult", "xxx", "porn", "18+", "sex", "uncensored", "exotic"]

# ==========================================
# 1. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Ù‚Ø© (Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ø­Ø¯Ø«Ø©)
# ==========================================
URLS = [
    # Ù‚ÙˆØ§Ø¦Ù… Ø±Ø³Ù…ÙŠØ© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
    "https://iptv-org.github.io/iptv/countries/jo.m3u",
    "https://iptv-org.github.io/iptv/countries/eg.m3u",
    "https://iptv-org.github.io/iptv/countries/sa.m3u",
    "https://iptv-org.github.io/iptv/countries/ae.m3u",
    "https://iptv-org.github.io/iptv/countries/kw.m3u",
    
    # Ù‚ÙˆØ§Ø¦Ù… Ø¶Ø®Ù…Ø© (Ù…Ø¬ØªÙ…Ø¹)
    "https://iptv-org.github.io/iptv/languages/ara.m3u",
    "https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8",
    "https://raw.githubusercontent.com/jnk22/kodirpo/master/iptv/arab.m3u",
    
    # Ù…Ù†ØµØ§Øª Ø¹Ø§Ù„Ù…ÙŠØ© (Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©)
    "https://i.mjh.nz/SamsungTVPlus/all.m3u8",
    "https://i.mjh.nz/PlutoTV/all.m3u8",
    
    # ØªØµÙ†ÙŠÙØ§Øª
    "https://iptv-org.github.io/iptv/categories/documentary.m3u",
    "https://iptv-org.github.io/iptv/categories/kids.m3u",
    "https://iptv-org.github.io/iptv/categories/sports.m3u",
    "https://iptv-org.github.io/iptv/categories/movies.m3u",
    "https://iptv-org.github.io/iptv/categories/religious.m3u"
]

# Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© (ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ù‚Ù†ÙˆØ§Øª Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆÙˆØ«Ø§Ø¦Ù‚ÙŠØ©)
TARGETS = [
    "mbc", "bein", "osn", "rotana", "art ", "shahid", "alkass", "ssc", "abudhabi", "dubai",
    "national geo", "nat geo", "discovery", "animal planet", "history", "tlc", "investigation",
    "spacetoon", "cartoon network", "cn arabia", "nickelodeon", "nick", "disney", "majid",
    "jordan", "roya", "mamlaka", "jazeera", "alarabiya", "skynews", "bbc",
    "samsung", "pluto", "rakuten", "quran", "sunnah", "iqraa", "majalis"
]

# ==========================================
# 3. Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø°ÙƒÙŠØ© (Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯)
# ==========================================

def check_stream(url):
    """
    ÙØ­Øµ Ø°ÙƒÙŠ Ø¬Ø¯Ø§Ù‹: ÙŠÙ‚ÙŠØ³ Ø§Ù„Ø³Ø±Ø¹Ø© (Latency) ÙˆÙŠØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰.
    ÙŠØ±Ø¬Ø¹: (Ù‡Ù„ ÙŠØ¹Ù…Ù„ØŸ, Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©)
    """
    start_time = time.time()
    try:
        # Ø§Ø®ØªÙŠØ§Ø± Ù‡ÙˆÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        agent = random.choice(USER_AGENTS)
        headers = {"User-Agent": agent}
        
        with requests.get(url, headers=headers, stream=True, timeout=TIMEOUT) as r:
            if r.status_code == 200:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ø£Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„Ø§Ù‹
                ctype = r.headers.get('Content-Type', '').lower()
                if any(x in ctype for x in ['video', 'mpegurl', 'octet-stream', 'application/x-mpegurl']):
                    latency = time.time() - start_time
                    return (True, latency)
    except:
        pass
    return (False, 999)

def extract_quality(name):
    name = name.upper()
    if "4K" in name: return "4K"
    if "FHD" in name: return "FHD"
    if "HD" in name: return "HD"
    return "SD"

def clean_name(name):
    original = name
    name = name.lower()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡
    if any(b in name for b in BLACKLIST):
        return None # Ø±ÙØ¶ Ø§Ù„Ù‚Ù†Ø§Ø©

    # ØªÙ†Ø¸ÙŠÙ
    junk = ["hd", "sd", "fhd", "4k", "hevc", "ar", "arabic", "tv", "channel", "live", "stream", "+", "(", ")", "[", "]", "|"]
    for w in junk:
        name = name.replace(w, "")
    
    name = re.sub(r'[^a-z0-9]', '', name)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (Mapping)
    mappings = {
        "mbc": ["mbc1", "mbcone"],
        "mbcdrama": ["mbcdrama"],
        "mbcaction": ["mbcaction"],
        "mbc2": ["mbc2", "mbctwo"],
        "mbc3": ["mbc3", "mbcthree"],
        "mbc4": ["mbc4", "mbcfour"],
        "mbcmasr": ["mbcmasr"],
        "mbciraq": ["mbciraq"],
        "mbcbollywood": ["mbcbollywood", "mbcbooly"],
        "natgeo": ["nationalgeographic", "natgeo", "nationalgeo"],
        "natgeowild": ["natgeowild", "wild"],
        "jordantv": ["jordan", "aljordon", "alurdun"],
        "roya": ["roya"],
        "almamlaka": ["almamlaka", "mamlaka"],
        "spacetoon": ["spacetoon"],
        "beinsports": ["bein", "beinsport"],
        "rotanacinema": ["rotanacinema"],
        "qurankareem": ["quran", "makkah"],
        "sunnah": ["sunnah", "madinah"]
    }
    
    for unified, variants in mappings.items():
        if any(v in name for v in variants):
            return unified
            
    return name

def get_cat(name, url=""):
    n = name.lower()
    u = url.lower()
    
    if "samsung" in u: return "samsung"
    if "quran" in n or "sunnah" in n or "iqraa" in n or "islam" in n: return "religious"
    if "sport" in n or "koora" in n or "bein" in n or "alkass" in n or "ssc" in n: return "sports"
    if "news" in n or "jazeera" in n or "arabia" in n or "bbc" in n or "sky" in n: return "news"
    if "kid" in n or "cartoon" in n or "spacetoon" in n or "nick" in n or "disney" in n: return "kids"
    if "movi" in n or "cinema" in n or "film" in n or "rotana" in n or "mbc 2" in n or "drama" in n: return "movies"
    if "docu" in n or "geo" in n or "wild" in n or "planet" in n or "history" in n: return "docu"
    
    return "general"

# ==========================================
# 4. Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ==========================================
def update():
    all_candidates = []
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ V6.0 (Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±Ø¹Ø© + Ø§Ù„ØªØ®ÙÙŠ)...")

    for url in URLS:
        try:
            resp = requests.get(url, timeout=20)
            resp.encoding = 'utf-8'
            lines = resp.text.split('\n')
            
            meta = {}
            for line in lines:
                line = line.strip()
                if line.startswith("#EXTINF"):
                    name_m = re.search(r'tvg-name="([^"]+)"', line) or re.search(r',(.*)', line)
                    name = name_m.group(1).strip() if name_m else "Unknown"
                    
                    logo_m = re.search(r'tvg-logo="([^"]+)"', line)
                    logo = logo_m.group(1) if logo_m else ""
                    
                    is_arab_list = "ara.m3u" in url or "jo.m3u" in url or "eg.m3u" in url or "sa.m3u" in url or "kw.m3u" in url
                    is_target_keyword = any(t in name.lower() for t in TARGETS)
                    
                    if is_arab_list or is_target_keyword:
                        meta = {"name": name, "logo": logo}
                    else:
                        meta = {} 
                        
                elif line.startswith("http") and meta:
                    if not line.endswith(".ts"):
                        all_candidates.append({
                            "name": meta['name'],
                            "logo": meta['logo'],
                            "url": line,
                            "quality": extract_quality(meta['name'])
                        })
                    meta = {}
        except Exception as e:
            print(f"âš ï¸ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ù…ØµØ¯Ø± {url}: {e}")

    print(f"ğŸ“¦ ØªÙ… ØªØ¬Ù…ÙŠØ¹ {len(all_candidates)} Ø±Ø§Ø¨Ø·. Ø¨Ø¯Ø¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø³Ø±Ø¹Ø©...")

    unique_links = set(c['url'] for c in all_candidates)
    # Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø³ÙŠØ­ÙØ¸: {Ø§Ù„Ø±Ø§Ø¨Ø·: Ø³Ø±Ø¹Ø©_Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©}
    working_links_stats = {}

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(check_stream, url): url for url in unique_links}
        
        checked = 0
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            checked += 1
            if checked % 100 == 0: print(f"âœ¨ ØªÙ… ÙØ­Øµ {checked}/{len(unique_links)}...")
            
            try:
                is_working, latency = future.result()
                if is_working:
                    working_links_stats[url] = latency
            except:
                pass

    print(f"âœ… ØªÙ… ØªØ£ÙƒÙŠØ¯ {len(working_links_stats)} Ø±Ø§Ø¨Ø·.")

    final_channels = {}
    
    for item in all_candidates:
        if item['url'] in working_links_stats:
            cid = clean_name(item['name'])
            
            if cid is None: continue # ØªÙ… Ø±ÙØ¶ Ø§Ù„Ù‚Ù†Ø§Ø© (Blacklist)

            if cid not in final_channels:
                display_name = item['name']
                if re.match(r'^[a-zA-Z0-9\s]+$', display_name): display_name = display_name.title()
                if "mbc" in cid and "drama" not in cid and "action" not in cid: display_name = display_name.upper()

                final_channels[cid] = {
                    "name": display_name,
                    "logo": item['logo'],
                    "category": get_cat(item['name'], item['url']),
                    "urls_stats": [], # Ø³Ù†Ø­ÙØ¸ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¹ Ø³Ø±Ø¹ØªÙ‡
                    "quality": item['quality']
                }
            
            # Ù†Ø¶ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ³Ø±Ø¹ØªÙ‡ Ù„Ù„Ù‚Ø§Ø¦Ù…Ø©
            # Ù†ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø§Ù„Ø±Ø§Ø¨Ø·
            if not any(u['url'] == item['url'] for u in final_channels[cid]['urls_stats']):
                final_channels[cid]['urls_stats'].append({
                    "url": item['url'],
                    "latency": working_links_stats[item['url']]
                })
                
                if not final_channels[cid]['logo'] and item['logo']:
                    final_channels[cid]['logo'] = item['logo']

    # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‡ÙŠÙƒÙ„
    output = []
    for cid, data in final_channels.items():
        if not data['urls_stats']: continue
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø© (Ø§Ù„Ø£Ø³Ø±Ø¹ Ø£ÙˆÙ„Ø§Ù‹)
        # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ø­Ø±! Ø³ÙŠØ¶Ø¹ Ø§Ù„Ø³ÙŠØ±ÙØ± Ø§Ù„Ø£Ø³Ø±Ø¹ ÙƒØ£ÙˆÙ„ Ø®ÙŠØ§Ø± Ù„Ù„Ù…Ø´ØºÙ„
        sorted_links = sorted(data['urls_stats'], key=lambda x: x['latency'])
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø· Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_urls = [x['url'] for x in sorted_links]
        
        output.append({
            "name": data['name'],
            "logo": data['logo'],
            "category": data['category'],
            "urls": final_urls,
            "quality": data['quality']
        })

    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª
    priority = ["Jordan", "Roya", "Mamlaka", "MBC", "Al Jazeera", "BeIN", "Nat Geo", "Quran"]
    def sort_logic(c):
        n = c['name'].lower()
        for i, p in enumerate(priority):
            if p.lower() in n: return i
        return 100

    output.sort(key=sort_logic)

    print(f"ğŸ‰ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²: {len(output)} Ù‚Ù†Ø§Ø©ØŒ Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø³Ø±Ø¹Ø© Ø§Ù„Ø³ÙŠØ±ÙØ±.")
    
    with open("channels.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    update()

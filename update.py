import requests
import json
import re
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse

# ==========================================
# ğŸ•·ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø²Ø§Ø­Ù Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØªÙŠ (V13 - HTTPS Priority)
# ==========================================
MAX_WORKERS = 50       # Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© Ù„Ù„ÙØ­Øµ
TIMEOUT = 5            # Ù…Ù‡Ù„Ø© Ù‚ØµÙŠØ±Ø© Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø§Ù„Ù…ÙŠØªØ© Ø¨Ø³Ø±Ø¹Ø©
MIN_CHANNELS = 15      # Ø£Ù…Ø§Ù† Ù„Ø¹Ø¯Ù… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù†Øª

# ØªÙ…ÙˆÙŠÙ‡ Ø§Ù„Ù…ØªØµÙØ­ (Ø¶Ø±ÙˆØ±ÙŠ Ø¬Ø¯Ø§Ù‹ Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¬Ø¨)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Safari/605.1.15",
    "VLC/3.0.20 LibVLC/3.0.20",
    "TiviMate/4.7.0"
]

# Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø©
BLACKLIST = ["adult", "xxx", "porn", "18+", "sex", "uncensored", "exotic", "hot", "xx"]

# ==========================================
# ğŸŒ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø²Ø­Ù (Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© ÙˆÙ…ØªØ¬Ø¯Ø¯Ø©)
# ==========================================
SEARCH_SOURCES = [
    # Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (GitHub Raw)
    "https://iptv-org.github.io/iptv/countries/jo.m3u",
    "https://iptv-org.github.io/iptv/countries/sa.m3u",
    "https://iptv-org.github.io/iptv/countries/ae.m3u",
    "https://iptv-org.github.io/iptv/countries/eg.m3u",
    "https://iptv-org.github.io/iptv/languages/ara.m3u",
    
    # Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ø£ÙˆØªÙˆÙ…Ø§ØªÙŠÙƒÙŠØ§Ù‹ (Raw Links - ÙƒÙ†Ø² Ù„Ù„Ù‚Ù†ÙˆØ§Øª)
    "https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8",
    "https://raw.githubusercontent.com/jnk22/kodirpo/master/iptv/arab.m3u",
    "https://raw.githubusercontent.com/yousf/tv/main/ar.m3u",
    "https://raw.githubusercontent.com/gielj/iptv-2/master/Ar.m3u",
    
    # Ù…ØµØ§Ø¯Ø± Ø¹Ø§Ù„Ù…ÙŠØ© Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚Ù†ÙˆØ§Øª Ø¹Ø±Ø¨ÙŠØ©
    "https://i.mjh.nz/SamsungTVPlus/all.m3u8",
    "https://i.mjh.nz/PlutoTV/all.m3u8"
]

# Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© (Ø§Ù„Ø£ÙƒØ«Ø± Ø·Ù„Ø¨Ø§Ù‹)
TARGETS = [
    "mbc", "bein", "osn", "rotana", "art ", "shahid", "alkass", "ssc", "abudhabi", "dubai",
    "jordan", "roya", "mamlaka", "jazeera", "alarabiya", "skynews",
    "national geo", "nat geo", "spacetoon", "cartoon network", "majid", "quran", "sunnah"
]

# Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø´Ø¹Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
LOGO_FIXER = {
    "mbc1": "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/MBC_1_Logo.svg/512px-MBC_1_Logo.svg.png",
    "mbc2": "https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/MBC_2_Logo.svg/512px-MBC_2_Logo.svg.png",
    "mbcaction": "https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/MBC_Action_Logo.svg/512px-MBC_Action_Logo.svg.png",
    "mbc3": "https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/MBC_3_Logo.svg/512px-MBC_3_Logo.svg.png",
    "roya": "https://upload.wikimedia.org/wikipedia/commons/7/77/Roya_TV_Logo.png",
    "almamlaka": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/AlMamlakaTV.svg/512px-AlMamlakaTV.svg.png",
    "beinsports": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/BeIN_Sports_logo.svg/512px-BeIN_Sports_logo.svg.png"
}

# ==========================================
# ğŸ§  ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø²Ø­Ù ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„
# ==========================================

def get_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://www.google.com/",
        "Accept": "*/*"
    }

def clean_name(name):
    """ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚Ù†ÙˆØ§Øª"""
    name = name.lower()
    if any(b in name for b in BLACKLIST): return None
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    junk = ["hd", "sd", "fhd", "4k", "hevc", "ar", "arabic", "tv", "live", "stream", "|", "[", "]", "(", ")", "vip", "new", "update", "channel"]
    for w in junk: name = name.replace(w, "")
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©
    name = name.strip(" .-0123456789")
    name = re.sub(r'[^a-z0-9]', '', name)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (Mapping) Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø§Ù„Ù‚Ù†Ø§Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ù…Ø®ØªÙ„ÙØ©
    maps = {
        "mbc1": ["mbc1", "mbcone"], 
        "mbc2": ["mbc2"], 
        "mbc3": ["mbc3"], 
        "mbc4": ["mbc4"],
        "mbcaction": ["mbcaction", "action"], 
        "mbcdrama": ["mbcdrama"], 
        "mbcmasr": ["mbcmasr"],
        "mbciraq": ["mbciraq"],
        "mbc5": ["mbc5"],
        "roya": ["roya"], 
        "almamlaka": ["mamlaka"], 
        "jordantv": ["jordan", "aljordon"],
        "spacetoon": ["spacetoon"], 
        "beinsports": ["bein", "beinsport"], 
        "rotanacinema": ["rotanacinema"], 
        "osn": ["osn"], 
        "art": ["artmovies", "arthekayat"]
    }
    
    for k, v in maps.items():
        if any(x in name for x in v): return k
    
    if len(name) < 2: return None
    return name

def check_stream(url):
    """ÙØ­Øµ Ø°ÙƒÙŠ Ù„Ù„Ø³ÙŠØ±ÙØ± Ù…Ø¹ ØªÙØ¶ÙŠÙ„ HTTPS"""
    start = time.time()
    try:
        # Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‚Ø§Ø·: Ù†Ø¹Ø·ÙŠ 50 Ù†Ù‚Ø·Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø¢Ù…Ù† HTTPS
        # Ù‡Ø°Ø§ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ø§Ø¨ØªÙˆØ¨ ÙˆØ§Ù„Ù…ØªØµÙØ­Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
        priority_bonus = 0
        if url.startswith("https"): priority_bonus = 50
        
        with requests.get(url, headers=get_headers(), stream=True, timeout=TIMEOUT, verify=False) as r:
            if r.status_code == 200:
                ct = r.headers.get('Content-Type', '').lower()
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                if any(t in ct for t in ['video', 'mpegurl', 'stream', 'octet', 'application/x-mpegurl']):
                    latency = time.time() - start
                    # Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: Ø§Ù„Ø³Ø±Ø¹Ø© - Ø§Ù„Ø¨ÙˆÙ†Øµ (ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„Ø±Ù‚Ù… ÙƒØ§Ù† Ø£ÙØ¶Ù„)
                    final_score = latency - (priority_bonus / 10) 
                    return (True, final_score)
    except:
        pass
    return (False, 999)

def fetch_and_parse():
    """Ø§Ù„Ø²Ø­Ù Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
    print("ğŸ•¸ï¸ Ø¨Ø¯Ø¡ Ø§Ù„Ø²Ø§Ø­Ù Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØªÙŠ...")
    found_streams = []

    def fetch_url(source_url):
        try:
            r = requests.get(source_url, headers=get_headers(), timeout=10, verify=False)
            if r.status_code == 200:
                return r.text
        except: return ""
        return ""

    # Ø¬Ù„Ø¨ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_url, SEARCH_SOURCES)

    for text in results:
        if not text: continue
        
        lines = text.split('\n')
        current_name = ""
        current_logo = ""
        
        for line in lines:
            line = line.strip()
            if not line: continue
            
            if line.startswith("#EXTINF"):
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù…Ø±ÙˆÙ†Ø©
                nm_match = re.search(r'tvg-name="([^"]+)"', line) or re.search(r',(.*)', line)
                lg_match = re.search(r'tvg-logo="([^"]+)"', line)
                
                if nm_match: 
                    raw_name = nm_match.group(1).split(',')[-1].strip()
                    # ØªÙ†Ø¸ÙŠÙ Ø£ÙˆÙ„ÙŠ Ø³Ø±ÙŠØ¹
                    current_name = raw_name
                
                if lg_match: current_logo = lg_match.group(1)
            
            elif line.startswith("http"):
                # ÙÙ„ØªØ±Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
                is_target = any(t in current_name.lower() for t in TARGETS)
                is_arabic = "ar" in line or "arab" in current_name.lower()
                
                if (is_target or is_arabic) and current_name:
                    found_streams.append({
                        "name": current_name,
                        "logo": current_logo,
                        "url": line
                    })
                
                current_name = "" 
                current_logo = ""

    print(f"ğŸ’° ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(found_streams)} Ø±Ø§Ø¨Ø·. Ø¬Ø§Ø±ÙŠ Ø§Ù„ÙØ­Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚...")
    return found_streams

def main():
    raw_data = fetch_and_parse()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
    unique_links = {item['url']: item for item in raw_data}
    urls_to_check = list(unique_links.keys())
    
    valid_channels = {}
    
    # ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
        futures = {exe.submit(check_stream, u): u for u in urls_to_check}
        
        for f in as_completed(futures):
            u = futures[f]
            try:
                is_working, score = f.result()
                if is_working:
                    item = unique_links[u]
                    cid = clean_name(item['name'])
                    if cid:
                        if cid not in valid_channels:
                            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø§Ø³Ù… Ù„Ù„Ø¹Ø±Ø¶
                            d_name = cid
                            # Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
                            if "mbc" in cid: d_name = cid.upper().replace("MBC", "MBC ")
                            elif "bein" in cid: d_name = "beIN Sports " + cid.replace("beinsports", "")
                            elif "roya" in cid: d_name = "Roya TV"
                            else: d_name = cid.title()
                            
                            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¹Ø§Ø± Ø«Ø§Ø¨Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¸Ù‡Ø±
                            logo = item['logo']
                            if cid in LOGO_FIXER: logo = LOGO_FIXER[cid]
                            elif not logo: logo = "https://via.placeholder.com/100?text=TV"

                            valid_channels[cid] = {
                                "name": d_name,
                                "logo": logo,
                                "category": "general",
                                "urls": []
                            }
                            
                            # ØªØµÙ†ÙŠÙ Ø¨Ø³ÙŠØ·
                            n_low = cid
                            if "sport" in n_low or "bein" in n_low: valid_channels[cid]["category"] = "sports"
                            elif "news" in n_low or "jazeera" in n_low: valid_channels[cid]["category"] = "news"
                            elif "kid" in n_low or "spacetoon" in n_low: valid_channels[cid]["category"] = "kids"
                            elif "quran" in n_low: valid_channels[cid]["category"] = "religious"
                            elif "movie" in n_low or "osn" in n_low: valid_channels[cid]["category"] = "movies"

                        valid_channels[cid]['urls'].append({"u": u, "s": score})
            except: pass

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    output = []
    for cid, data in valid_channels.items():
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø­Ø³Ø¨ Ø§Ù„Ø£ÙØ¶Ù„ÙŠØ© (Score Ø§Ù„Ø£Ù‚Ù„ Ù‡Ùˆ Ø§Ù„Ø£ÙØ¶Ù„)
        sorted_urls = sorted(data['urls'], key=lambda x: x['s'])
        final_urls = [x['u'] for x in sorted_urls[:8]] # Ù†Ø­ØªÙØ¸ Ø¨Ø£ÙØ¶Ù„ 8 Ø³ÙŠØ±ÙØ±Ø§Øª
        
        output.append({
            "name": data['name'],
            "logo": data['logo'],
            "category": data['category'],
            "urls": final_urls
        })

    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø¨Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
    prio_list = ["jordan", "roya", "mbc", "bein", "news"]
    output.sort(key=lambda x: next((i for i, p in enumerate(prio_list) if p in clean_name(x['name']) or ""), 99))

    if len(output) >= MIN_CHANNELS:
        with open("channels.json", "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ù†Ø¬Ø§Ø­! {len(output)} Ù‚Ù†Ø§Ø© ØªØ¹Ù…Ù„.")
    else:
        print("âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹ØŒ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù Ù„ØªØ¬Ù†Ø¨ Ø­Ø°Ù Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©.")

if __name__ == "__main__":
    main()

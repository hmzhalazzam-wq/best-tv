import requests
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# 1. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Ù‚Ø© (Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹Ø§Øª)
# ==========================================
URLS = [
    # --- Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ø±Ø³Ù…ÙŠØ© ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø© ---
    "https://iptv-org.github.io/iptv/countries/jo.m3u",
    "https://iptv-org.github.io/iptv/countries/eg.m3u",
    "https://iptv-org.github.io/iptv/countries/sa.m3u",
    "https://iptv-org.github.io/iptv/countries/ae.m3u",
    "https://iptv-org.github.io/iptv/countries/kw.m3u",
    
    # --- Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ¨Ø±Ù‰ (ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª) ---
    "https://iptv-org.github.io/iptv/languages/ara.m3u",
    "https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8",
    "https://raw.githubusercontent.com/jnk22/kodirpo/master/iptv/arab.m3u",
    
    # --- Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ù†ØµØ§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Samsung TV+, Pluto, Roku) ---
    # Ù‡Ø°Ù‡ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚Ù†ÙˆØ§Øª ÙˆØ«Ø§Ø¦Ù‚ÙŠØ© ÙˆØ£Ø·ÙØ§Ù„ Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹
    "https://i.mjh.nz/SamsungTVPlus/all.m3u8",
    "https://i.mjh.nz/PlutoTV/all.m3u8",
    
    # --- Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
    "https://iptv-org.github.io/iptv/categories/documentary.m3u",
    "https://iptv-org.github.io/iptv/categories/kids.m3u",
    "https://iptv-org.github.io/iptv/categories/sports.m3u",
    "https://iptv-org.github.io/iptv/categories/movies.m3u",
    "https://iptv-org.github.io/iptv/categories/music.m3u"
]

# Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ù‚Ù†ÙˆØ§Øª (Targets)
# ØªÙ… ØªÙˆØ³ÙŠØ¹Ù‡Ø§ Ù„ØªØ´Ù…Ù„ Ù‚Ù†ÙˆØ§Øª Ø£ÙƒØ«Ø± ØªÙ†ÙˆØ¹Ø§Ù‹
TARGETS = [
    # Ù‚Ù†ÙˆØ§Øª Ø¹Ø±Ø¨ÙŠØ© ÙƒØ¨Ø±Ù‰
    "mbc", "bein", "osn", "rotana", "art ", "shahid", "alkass", "ssc", "abudhabi", "dubai",
    # Ù‚Ù†ÙˆØ§Øª ÙˆØ«Ø§Ø¦Ù‚ÙŠØ© ÙˆØ¹Ø§Ù„Ù…ÙŠØ©
    "national geo", "nat geo", "discovery", "animal planet", "history", "tlc", "investigation",
    # Ø£Ø·ÙØ§Ù„
    "spacetoon", "cartoon network", "cn arabia", "nickelodeon", "nick", "disney", "majid",
    # Ø£Ø®Ø¨Ø§Ø± ÙˆÙ‚Ù†ÙˆØ§Øª ÙˆØ·Ù†ÙŠØ©
    "jordan", "roya", "mamlaka", "jazeera", "alarabiya", "skynews", "bbc",
    # Ù…Ù†ØµØ§Øª
    "samsung", "pluto", "rakuten"
]

# ==========================================
# 2. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙØ­Øµ
# ==========================================
MAX_WORKERS = 40  # Ø²Ø¯Ù†Ø§ Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ù„Ù„Ø³Ø±Ø¹Ø©
TIMEOUT = 5       # Ø²Ø¯Ù†Ø§ ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù„Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ==========================================
# 3. Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø°ÙƒÙŠØ©
# ==========================================

def check_stream(url):
    """ÙØ­Øµ Ø°ÙƒÙŠ: ÙŠØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ¹Ù…Ù„ ÙˆØ£Ù†Ù‡ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„Ø§Ù‹"""
    try:
        with requests.get(url, headers=HEADERS, stream=True, timeout=TIMEOUT) as r:
            if r.status_code == 200:
                # ØªØ­Ù‚Ù‚ Ø¥Ø¶Ø§ÙÙŠ: Ù‡Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙŠØ¯ÙŠÙˆØŸ
                content_type = r.headers.get('Content-Type', '').lower()
                if 'video' in content_type or 'mpegurl' in content_type or 'octet-stream' in content_type:
                    return True
    except:
        return False
    return False

def extract_quality(name):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ù† Ø§Ù„Ø§Ø³Ù…"""
    name = name.upper()
    if "4K" in name: return "4K"
    if "FHD" in name: return "FHD"
    if "HD" in name: return "HD"
    return "SD"

def clean_name(name):
    """ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡"""
    name = name.lower()
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    junk = ["hd", "sd", "fhd", "4k", "hevc", "ar", "arabic", "tv", "channel", "live", "stream", "+", "(", ")", "[", "]"]
    for w in junk:
        name = name.replace(w, "")
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ²
    name = re.sub(r'[^a-z0-9]', '', name)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ù…ÙŠØ§Øª (Mapping)
    mappings = {
        "mbc": ["mbc1", "mbcone"],
        "mbcdrama": ["mbcdrama"],
        "mbcaction": ["mbcaction"],
        "mbc2": ["mbc2", "mbctwo"],
        "mbc3": ["mbc3", "mbcthree"],
        "mbc4": ["mbc4", "mbcfour"],
        "mbcmasr": ["mbcmasr"],
        "mbciraq": ["mbciraq"],
        "mbcbollywood": ["mbcbollywood", "mbcbooly"],
        "natgeo": ["nationalgeographic", "natgeo", "nationalgeo"],
        "natgeowild": ["natgeowild", "wild"],
        "jordantv": ["jordan", "aljordon", "alurdun"],
        "roya": ["roya"],
        "almamlaka": ["almamlaka", "mamlaka"],
        "spacetoon": ["spacetoon"],
        "beinsports": ["bein", "beinsport"],
        "rotanacinema": ["rotanacinema"],
        "rotanaclassic": ["rotanaclassic"]
    }
    
    for unified, variants in mappings.items():
        if any(v in name for v in variants):
            return unified
            
    return name

def get_cat(name, url=""):
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„Ø±Ø§Ø¨Ø·"""
    n = name.lower()
    u = url.lower()
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø· (Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©)
    if "samsung" in u: return "samsung"
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³Ù…
    if "sport" in n or "koora" in n or "bein" in n or "alkass" in n or "ssc" in n: return "sports"
    if "news" in n or "jazeera" in n or "arabia" in n or "bbc" in n or "sky" in n: return "news"
    if "kid" in n or "cartoon" in n or "spacetoon" in n or "nick" in n or "disney" in n: return "kids"
    if "movi" in n or "cinema" in n or "film" in n or "rotana" in n or "mbc 2" in n or "drama" in n: return "movies"
    if "docu" in n or "geo" in n or "wild" in n or "planet" in n or "history" in n: return "docu"
    if "music" in n or "radio" in n: return "music"
    if "quran" in n or "sunna" in n: return "religious"
    
    return "general"

# ==========================================
# 4. Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ==========================================
def update():
    all_candidates = []
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ (V5.0)... Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© ÙˆØ§Ù„Ø±Ø³Ù…ÙŠØ©...")

    for url in URLS:
        try:
            resp = requests.get(url, timeout=20) # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆÙ‚Øª Ù„Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
            resp.encoding = 'utf-8'
            lines = resp.text.split('\n')
            
            meta = {}
            for line in lines:
                line = line.strip()
                if line.startswith("#EXTINF"):
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³Ù…
                    name_m = re.search(r'tvg-name="([^"]+)"', line) or re.search(r',(.*)', line)
                    name = name_m.group(1).strip() if name_m else "Unknown"
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø¹Ø§Ø±
                    logo_m = re.search(r'tvg-logo="([^"]+)"', line)
                    logo = logo_m.group(1) if logo_m else ""
                    
                    # ÙÙ„ØªØ±Ø© Ø°ÙƒÙŠØ©
                    is_arab_list = "ara.m3u" in url or "jo.m3u" in url or "eg.m3u" in url or "sa.m3u" in url
                    is_target_keyword = any(t in name.lower() for t in TARGETS)
                    
                    # Ù†Ù‚Ø¨Ù„ Ø§Ù„Ù‚Ù†Ø§Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø¹Ø±Ø¨ÙŠØ©ØŒ Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³Ù…Ù‡Ø§ Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
                    if is_arab_list or is_target_keyword:
                        meta = {"name": name, "logo": logo}
                    else:
                        meta = {} 
                        
                elif line.startswith("http") and meta:
                    if not line.endswith(".ts"):
                        all_candidates.append({
                            "name": meta['name'],
                            "logo": meta['logo'],
                            "url": line,
                            "quality": extract_quality(meta['name']) # Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©
                        })
                    meta = {}
        except Exception as e:
            print(f"âš ï¸ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ù…ØµØ¯Ø± {url}: {e}")

    print(f"ğŸ“¦ ØªÙ… ØªØ¬Ù…ÙŠØ¹ {len(all_candidates)} Ø±Ø§Ø¨Ø·. Ø¨Ø¯Ø¡ Ø§Ù„ÙØ­Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚...")

    # ØªØ¬Ù…ÙŠØ¹ ÙˆÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    unique_links = set(c['url'] for c in all_candidates)
    working_links = set()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(check_stream, url): url for url in unique_links}
        
        checked = 0
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            checked += 1
            if checked % 100 == 0: print(f"âœ¨ ØªÙ… ÙØ­Øµ {checked}/{len(unique_links)}...")
            
            try:
                if future.result():
                    working_links.add(url)
            except:
                pass

    print(f"âœ… ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø¹Ù…Ù„ {len(working_links)} Ø±Ø§Ø¨Ø·.")

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_channels = {}
    
    for item in all_candidates:
        if item['url'] in working_links:
            cid = clean_name(item['name'])
            
            if cid not in final_channels:
                # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³Ù… Ù„Ù„Ø¹Ø±Ø¶
                display_name = item['name']
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³Ù… Ø¨Ø§Ù„Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠØŒ Ù†Ø¬Ø¹Ù„Ù‡ Capitalized
                if re.match(r'^[a-zA-Z0-9\s]+$', display_name):
                    display_name = display_name.title()
                
                # ØªØµØ­ÙŠØ­ Ø®Ø§Øµ Ù„Ù€ MBC
                if "mbc" in cid and "drama" not in cid and "action" not in cid: display_name = display_name.upper()

                final_channels[cid] = {
                    "name": display_name,
                    "logo": item['logo'],
                    "category": get_cat(item['name'], item['url']),
                    "urls": [],
                    "quality": item['quality']
                }
            
            if item['url'] not in final_channels[cid]['urls']:
                final_channels[cid]['urls'].append(item['url'])
                if not final_channels[cid]['logo'] and item['logo']:
                    final_channels[cid]['logo'] = item['logo']

    # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    output = list(final_channels.values())
    output = [c for c in output if c['urls']]

    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª (Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© ÙˆØ§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹)
    priority = ["Jordan", "Roya", "Mamlaka", "MBC", "Al Jazeera", "BeIN", "Nat Geo"]
    def sort_logic(c):
        n = c['name'].lower()
        for i, p in enumerate(priority):
            if p.lower() in n: return i
        return 100

    output.sort(key=sort_logic)

    print(f"ğŸ‰ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²: {len(output)} Ù‚Ù†Ø§Ø© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø¨Ø«.")
    
    with open("channels.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    update()

import requests
import json
import re
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# 0. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ (Engine Settings)
# ==========================================
MAX_WORKERS = 60      # Ø³Ø±Ø¹Ø© Ù‚ØµÙˆÙ‰ (Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©)
TIMEOUT = 5           # Ù…Ù‡Ù„Ø© ÙØ­Øµ Ø§Ù„Ø±Ø§Ø¨Ø·

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ®ÙÙŠ (Ù„Ø®Ø¯Ø§Ø¹ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "VLC/3.0.20 LibVLC/3.0.20",
    "Kodi/19.0 (Matrix) Libreelec/10.0",
    "IPTV Smarters Pro"
]

BLACKLIST = ["adult", "xxx", "porn", "18+", "sex", "uncensored", "exotic", "babes"]

# ==========================================
# 1. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø°ÙƒÙŠØ© (Smart Sources)
# ==========================================
URLS = [
    # --- Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø±Ø³Ù…ÙŠØ© (Ø­Ø¬Ø± Ø§Ù„Ø£Ø³Ø§Ø³) ---
    "https://iptv-org.github.io/iptv/countries/jo.m3u", # Ø§Ù„Ø£Ø±Ø¯Ù†
    "https://iptv-org.github.io/iptv/countries/eg.m3u", # Ù…ØµØ±
    "https://iptv-org.github.io/iptv/countries/sa.m3u", # Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
    "https://iptv-org.github.io/iptv/countries/ae.m3u", # Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª
    "https://iptv-org.github.io/iptv/countries/kw.m3u", # Ø§Ù„ÙƒÙˆÙŠØª
    "https://iptv-org.github.io/iptv/countries/lb.m3u", # Ù„Ø¨Ù†Ø§Ù†
    
    # --- Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ (Community Lists) - ÙƒÙ†Ø² Ø§Ù„Ù‚Ù†ÙˆØ§Øª ---
    "https://iptv-org.github.io/iptv/languages/ara.m3u",
    "https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8",
    "https://raw.githubusercontent.com/jnk22/kodirpo/master/iptv/arab.m3u",
    "https://raw.githubusercontent.com/vthanby/EPG/main/lists/arabic.m3u", # Ù…ØµØ¯Ø± Ø¬Ø¯ÙŠØ¯ Ù‚ÙˆÙŠ
    
    # --- Ø§Ù„Ù…Ù†ØµØ§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Samsung TV+, Pluto) ---
    "https://i.mjh.nz/SamsungTVPlus/all.m3u8",
    "https://i.mjh.nz/PlutoTV/all.m3u8",
    
    # --- Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø®Ø§ØµØ© ---
    "https://iptv-org.github.io/iptv/categories/kids.m3u",
    "https://iptv-org.github.io/iptv/categories/sports.m3u",
    "https://iptv-org.github.io/iptv/categories/documentary.m3u",
    "https://iptv-org.github.io/iptv/categories/religious.m3u"
]

# Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Targets)
TARGETS = [
    # Ø¹Ø±Ø¨ÙŠØ©
    "mbc", "bein", "osn", "rotana", "art ", "shahid", "alkass", "ssc", "abudhabi", "dubai", "sharjah",
    "jordan", "roya", "mamlaka", "jazeera", "alarabiya", "skynews", "bbc", "alhurra", "ltv", "mtv",
    # Ø¹Ø§Ù„Ù…ÙŠØ©
    "national geo", "nat geo", "discovery", "animal planet", "history", "tlc", "investigation", "nasa",
    # Ø£Ø·ÙØ§Ù„
    "spacetoon", "cartoon network", "cn arabia", "nickelodeon", "nick", "disney", "majid", "baraem",
    # Ø¯ÙŠÙ†ÙŠØ©
    "quran", "sunnah", "iqraa", "majalis", "resala"
]

# ==========================================
# 2. Ø·Ø¨ÙŠØ¨ Ø§Ù„Ø´Ø¹Ø§Ø±Ø§Øª (Logo Doctor)
# ==========================================
# Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØµÙˆØ± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¯Ù‚Ø© Ù„Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø© (ÙÙŠ Ø­Ø§Ù„ Ø¹Ø¯Ù… ØªÙˆÙØ±Ù‡Ø§)
LOGO_FIXER = {
    "mbc1": "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/MBC_1_Logo.svg/512px-MBC_1_Logo.svg.png",
    "mbc2": "https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/MBC_2_Logo.svg/512px-MBC_2_Logo.svg.png",
    "mbc3": "https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/MBC_3_Logo.svg/512px-MBC_3_Logo.svg.png",
    "mbc4": "https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/MBC_4_Logo.svg/512px-MBC_4_Logo.svg.png",
    "mbcaction": "https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/MBC_Action_Logo.svg/512px-MBC_Action_Logo.svg.png",
    "mbcmax": "https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/MBC_Max_Logo.svg/512px-MBC_Max_Logo.svg.png",
    "mbcdrama": "https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/MBC_Drama_Logo.svg/512px-MBC_Drama_Logo.svg.png",
    "mbcmasr": "https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/MBC_Masr_Logo.svg/512px-MBC_Masr_Logo.svg.png",
    "roya": "https://upload.wikimedia.org/wikipedia/commons/7/77/Roya_TV_Logo.png",
    "almamlaka": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/AlMamlakaTV.svg/512px-AlMamlakaTV.svg.png",
    "jordantv": "https://upload.wikimedia.org/wikipedia/en/2/22/Jordan_Radio_and_Television_Corporation_logo.png",
    "aljazeera": "https://upload.wikimedia.org/wikipedia/en/thumb/f/f2/Aljazeera_eng.svg/512px-Aljazeera_eng.svg.png",
    "alarabiya": "https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Al_Arabiya.svg/512px-Al_Arabiya.svg.png",
    "spacetoon": "https://upload.wikimedia.org/wikipedia/ar/d/d4/Spacetoon_logo_2015.png",
    "rotanacinema": "https://upload.wikimedia.org/wikipedia/commons/8/88/Rotana_Cinema_Logo.png",
    "natgeo": "https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/National_Geographic_Logo.svg/512px-National_Geographic_Logo.svg.png",
    "beinsports": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/BeIN_Sports_logo.svg/512px-BeIN_Sports_logo.svg.png"
}

# ==========================================
# 3. Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø°ÙƒÙŠ (Smart Logic)
# ==========================================

def check_stream(url):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø°ÙƒÙŠ: Ø³Ø±Ø¹Ø© + Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
    start_time = time.time()
    try:
        agent = random.choice(USER_AGENTS)
        headers = {"User-Agent": agent}
        
        with requests.get(url, headers=headers, stream=True, timeout=TIMEOUT) as r:
            if r.status_code == 200:
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ùˆ ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ØµÙØ­Ø© Ø®Ø·Ø£
                ctype = r.headers.get('Content-Type', '').lower()
                valid_types = ['video', 'mpegurl', 'octet-stream', 'application/x-mpegurl', 'vnd.apple.mpegurl']
                
                if any(t in ctype for t in valid_types):
                    latency = time.time() - start_time
                    return (True, latency)
    except:
        pass
    return (False, 999)

def extract_quality(name):
    name = name.upper()
    if "4K" in name or "UHD" in name: return "4K"
    if "FHD" in name: return "FHD"
    if "HD" in name: return "HD"
    return "SD"

def clean_name(name):
    """ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©"""
    original = name
    name = name.lower()
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ¡
    if any(b in name for b in BLACKLIST): return None

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    junk = ["hd", "sd", "fhd", "4k", "hevc", "ar", "arabic", "tv", "channel", "live", "stream", "+", "(", ")", "[", "]", "|", "usa:", "uk:"]
    for w in junk:
        name = name.replace(w, "")
    
    name = re.sub(r'[^a-z0-9]', '', name) # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø·
    
    # Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ© Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (Mapping)
    mappings = {
        "mbc1": ["mbc1", "mbcone"],
        "mbc2": ["mbc2", "mbctwo"],
        "mbc3": ["mbc3"],
        "mbc4": ["mbc4"],
        "mbc5": ["mbc5"],
        "mbcdrama": ["mbcdrama"],
        "mbcaction": ["mbcaction"],
        "mbcmax": ["mbcmax"],
        "mbcmasr": ["mbcmasr"],
        "mbcmasr2": ["mbcmasr2"],
        "mbciraq": ["mbciraq"],
        "mbcbollywood": ["mbcbollywood", "mbcbooly"],
        "natgeo": ["nationalgeographic", "natgeo", "nationalgeo"],
        "natgeowild": ["natgeowild", "wild"],
        "natgeokids": ["natgeokids"],
        "jordantv": ["jordan", "aljordon", "alurdun"],
        "roya": ["roya"],
        "almamlaka": ["almamlaka", "mamlaka"],
        "spacetoon": ["spacetoon"],
        "cartoonnetwork": ["cartoonnetwork", "cn"],
        "cnarabia": ["cnarabia", "cartoonnetworkarabic"],
        "beinsports": ["bein", "beinsport"],
        "beinsportsnews": ["beinnews", "beinsportnews"],
        "rotanacinema": ["rotanacinema"],
        "rotanaclassic": ["rotanaclassic"],
        "rotanacomedy": ["rotanacomedy"],
        "rotanamusic": ["rotanamusic"],
        "qurankareem": ["quran", "makkah"],
        "sunnah": ["sunnah", "madinah"]
    }
    
    for unified, variants in mappings.items():
        if any(v in name for v in variants):
            return unified
            
    # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŒ Ù†Ø¹ÙŠØ¯ Ø§Ù„Ø§Ø³Ù… Ù…Ù†Ø¸ÙØ§Ù‹
    return name

def get_cat(name, url=""):
    n = name.lower()
    u = url.lower()
    
    if "samsung" in u: return "samsung"
    if "pluto" in u: return "movies"
    if "quran" in n or "sunnah" in n or "iqraa" in n or "islam" in n or "majalis" in n: return "religious"
    if "sport" in n or "koora" in n or "bein" in n or "alkass" in n or "ssc" in n or "ad sport" in n: return "sports"
    if "news" in n or "jazeera" in n or "arabia" in n or "bbc" in n or "sky" in n or "alhurra" in n: return "news"
    if "kid" in n or "cartoon" in n or "spacetoon" in n or "nick" in n or "disney" in n or "majid" in n: return "kids"
    if "movi" in n or "cinema" in n or "film" in n or "rotana" in n or "mbc 2" in n or "drama" in n or "action" in n: return "movies"
    if "docu" in n or "geo" in n or "wild" in n or "planet" in n or "history" in n: return "docu"
    
    return "general"

# ==========================================
# 4. Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Main Execution)
# ==========================================
def update():
    all_candidates = []
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ V7.0 (Ù…Ø¹ Ù…ÙŠØ²Ø© Ø·Ø¨ÙŠØ¨ Ø§Ù„Ø´Ø¹Ø§Ø±Ø§Øª)...")

    # 1. Ø¬Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    for url in URLS:
        try:
            resp = requests.get(url, timeout=20)
            resp.encoding = 'utf-8'
            lines = resp.text.split('\n')
            
            meta = {}
            for line in lines:
                line = line.strip()
                if line.startswith("#EXTINF"):
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³Ù…
                    name_m = re.search(r'tvg-name="([^"]+)"', line) or re.search(r',(.*)', line)
                    name = name_m.group(1).strip() if name_m else "Unknown"
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø¹Ø§Ø±
                    logo_m = re.search(r'tvg-logo="([^"]+)"', line)
                    logo = logo_m.group(1) if logo_m else ""
                    
                    # Ø§Ù„ÙÙ„ØªØ±Ø©: Ù‡Ù„ Ø§Ù„Ù‚Ù†Ø§Ø© Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ù…Ø³ØªÙ‡Ø¯ÙØ©ØŸ
                    is_arab = "ara.m3u" in url or "jo.m3u" in url or "eg.m3u" in url or "sa.m3u" in url or "kw.m3u" in url or "lb.m3u" in url
                    is_target = any(t in name.lower() for t in TARGETS)
                    
                    if is_arab or is_target:
                        meta = {"name": name, "logo": logo}
                    else:
                        meta = {} 
                        
                elif line.startswith("http") and meta:
                    if not line.endswith(".ts"):
                        all_candidates.append({
                            "name": meta['name'],
                            "logo": meta['logo'],
                            "url": line,
                            "quality": extract_quality(meta['name'])
                        })
                    meta = {}
        except Exception as e:
            print(f"âš ï¸ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ù…ØµØ¯Ø± {url}: {e}")

    print(f"ğŸ“¦ ØªÙ… ØªØ¬Ù…ÙŠØ¹ {len(all_candidates)} Ø±Ø§Ø¨Ø· Ù…Ø­ØªÙ…Ù„. Ø¨Ø¯Ø¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø³Ø±Ø¹Ø©...")

    # 2. ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (Multi-threading)
    unique_links = set(c['url'] for c in all_candidates)
    working_stats = {}

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(check_stream, url): url for url in unique_links}
        
        checked = 0
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            checked += 1
            if checked % 100 == 0: print(f"âœ¨ ØªÙ… ÙØ­Øµ {checked}/{len(unique_links)}...")
            
            try:
                is_working, latency = future.result()
                if is_working:
                    working_stats[url] = latency
            except:
                pass

    print(f"âœ… ØªÙ… ØªØ£ÙƒÙŠØ¯ {len(working_stats)} Ø±Ø§Ø¨Ø·.")

    # 3. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    final_channels = {}
    
    for item in all_candidates:
        if item['url'] in working_stats:
            cid = clean_name(item['name'])
            
            if cid is None: continue 

            if cid not in final_channels:
                # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³Ù… Ù„Ù„Ø¹Ø±Ø¶
                display_name = item['name']
                if re.match(r'^[a-zA-Z0-9\s]+$', display_name): display_name = display_name.title()
                if "mbc" in cid and "drama" not in cid and "action" not in cid: display_name = display_name.upper()

                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø¨ÙŠØ¨ Ø§Ù„Ø´Ø¹Ø§Ø±Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø´Ø¹Ø§Ø±
                final_logo = item['logo']
                if (not final_logo or len(final_logo) < 5) and cid in LOGO_FIXER:
                    final_logo = LOGO_FIXER[cid]

                final_channels[cid] = {
                    "name": display_name,
                    "logo": final_logo,
                    "category": get_cat(item['name'], item['url']),
                    "urls_stats": [],
                    "quality": item['quality']
                }
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø§Ø¨Ø· (Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙƒØ±Ø±Ø§Ù‹)
            if not any(u['url'] == item['url'] for u in final_channels[cid]['urls_stats']):
                final_channels[cid]['urls_stats'].append({
                    "url": item['url'],
                    "latency": working_stats[item['url']]
                })
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø´Ø¹Ø§Ø± Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø´Ø¹Ø§Ø±Ø§Ù‹ Ø£ÙØ¶Ù„ ÙÙŠ Ù…ØµØ¯Ø± Ø¢Ø®Ø±
                if not final_channels[cid]['logo'] and item['logo']:
                     final_channels[cid]['logo'] = item['logo']
                # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø´Ø¹Ø§Ø±Ø§Ù‹ ÙÙŠ "Ø·Ø¨ÙŠØ¨ Ø§Ù„Ø´Ø¹Ø§Ø±Ø§Øª"ØŒ Ù†Ø¹ØªÙ…Ø¯Ù‡ ÙÙˆØ±Ø§Ù‹ Ù„Ø£Ù†Ù‡ Ø§Ù„Ø£ÙØ¶Ù„
                if cid in LOGO_FIXER:
                    final_channels[cid]['logo'] = LOGO_FIXER[cid]

    # 4. Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ØªØµØ¯ÙŠØ±
    output = []
    stats_counter = {} # Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª

    for cid, data in final_channels.items():
        if not data['urls_stats']: continue
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø© (Ø§Ù„Ø£Ø³Ø±Ø¹ Ø£ÙˆÙ„Ø§Ù‹)
        sorted_links = sorted(data['urls_stats'], key=lambda x: x['latency'])
        final_urls = [x['url'] for x in sorted_links]
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        cat = data['category']
        stats_counter[cat] = stats_counter.get(cat, 0) + 1
        
        output.append({
            "name": data['name'],
            "logo": data['logo'],
            "category": cat,
            "urls": final_urls,
            "quality": data['quality']
        })

    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª
    priority = ["Jordan", "Roya", "Mamlaka", "MBC", "Al Jazeera", "BeIN", "Nat Geo", "Quran"]
    def sort_logic(c):
        n = c['name'].lower()
        for i, p in enumerate(priority):
            if p.lower() in n: return i
        return 100

    output.sort(key=sort_logic)

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    print("-" * 30)
    print("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
    for cat, count in stats_counter.items():
        print(f"   - {cat}: {count} Ù‚Ù†Ø§Ø©")
    print("-" * 30)
    print(f"ğŸ‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„ÙƒÙ„ÙŠ: {len(output)} Ù‚Ù†Ø§Ø© Ø¬Ø§Ù‡Ø²Ø©.")
    
    with open("channels.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    update()
